{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition of a set of 702783 objects. Total size = 1204437943 bytes.\n",
      " Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)\n",
      "     0      2   0 558366640  46 558366640  46 pandas.core.frame.DataFrame\n",
      "     1    179   0 538667532  45 1097034172  91 numpy.ndarray\n",
      "     2 199392  28 27267757   2 1124301929  93 str\n",
      "     3      2   0 19941712   2 1144243641  95 pandas.core.indexes.numeric.Int64Index\n",
      "     4 182581  26 14614784   1 1158858425  96 tuple\n",
      "     5  83153  12  6555106   1 1165413531  97 bytes\n",
      "     6  42012   6  6078680   1 1171492211  97 types.CodeType\n",
      "     7  38761   6  5271496   0 1176763707  98 function\n",
      "     8   5151   1  5192336   0 1181956043  98 type\n",
      "     9   9836   1  3648008   0 1185604051  98 dict (no owner)\n",
      "<1778 more rows. Type e.g. '_.more' to view.>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "BIN = '../'\n",
    "sys.path.append(BIN)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "import my_matplotlib_style as ms\n",
    "from scipy import stats\n",
    "import utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from fastai.callbacks.tracker import SaveModelCallback\n",
    "\n",
    "from fastai import basic_train, basic_data\n",
    "from fastai.callbacks import ActivationStats\n",
    "from fastai import train as tr\n",
    "\n",
    "from nn_utils import get_data, RMSELoss\n",
    "from utils import plot_activations\n",
    "\n",
    "from nn_utils import AE_basic, AE_bn_LeakyReLU\n",
    "\n",
    "mpl.rc_file(BIN + 'my_matplotlib_rcparams')\n",
    "\n",
    "from guppy import hpy; hp=hpy()\n",
    "\n",
    "# Load data\n",
    "train = pd.read_pickle(BIN + 'processed_data/aod/all_jets_train_27D_5_percent.pkl')\n",
    "test = pd.read_pickle(BIN + 'processed_data/aod/all_jets_test_27D_5_percent.pkl')\n",
    "\n",
    "#Remove irrelevant columns\n",
    "#train.pop('JetGhostArea')\n",
    "#test.pop('JetGhostArea')\n",
    "#train.pop('BchCorrCell')\n",
    "#test.pop('BchCorrCell')\n",
    "\n",
    "# Remove extreme/bad jets\n",
    "train = utils.filter_jets(train)\n",
    "test = utils.filter_jets(test)\n",
    "\n",
    "# Normalize\n",
    "train_mean = train.mean()\n",
    "train_std = train.std()\n",
    "\n",
    "train, test = utils.custom_normalization(train, test)\n",
    "\n",
    "train_x = train\n",
    "test_x = test\n",
    "train_y = train_x  # y = x since we are building and AE\n",
    "test_y = test_x\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(train_x.values), torch.tensor(train_y.values))\n",
    "valid_ds = TensorDataset(torch.tensor(test_x.values), torch.tensor(test_y.values))\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs=1024)\n",
    "db = basic_data.DataBunch(train_dl, valid_dl)\n",
    "\n",
    "module_name = 'AE_bn_LeakyReLU'\n",
    "module = AE_bn_LeakyReLU\n",
    "grid_search_folder = \"AE_bn_LeakyReLU_AOD_grid_search_custom_normalization_1500epochs/\"\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "#The folder to analyse\n",
    "model_folder_name = \"AE_27_200_200_200_18_200_200_200_27\"\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "print(hp.heap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE_27_200_200_200_18_200_200_200_27 best_AE_bn_LeakyReLU_bs4096_lr1e-02_wd1e-02\n",
      "Partition of a set of 703211 objects. Total size = 1204473585 bytes.\n",
      " Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)\n",
      "     0      2   0 558366640  46 558366640  46 pandas.core.frame.DataFrame\n",
      "     1    179   0 538667532  45 1097034172  91 numpy.ndarray\n",
      "     2 199333  28 27258847   2 1124293019  93 str\n",
      "     3      2   0 19941712   2 1144234731  95 pandas.core.indexes.numeric.Int64Index\n",
      "     4 182731  26 14627136   1 1158861867  96 tuple\n",
      "     5  83160  12  6555630   1 1165417497  97 bytes\n",
      "     6  42016   6  6079256   1 1171496753  97 types.CodeType\n",
      "     7  38903   6  5290808   0 1176787561  98 function\n",
      "     8   5151   1  5192336   0 1181979897  98 type\n",
      "     9   9833   1  3649000   0 1185628897  98 dict (no owner)\n",
      "<1795 more rows. Type e.g. '_.more' to view.>\n"
     ]
    }
   ],
   "source": [
    "#Just alter this if you want to iterate through every model\n",
    "for model_folder in [x for x in os.scandir(grid_search_folder) if x.name == model_folder_name]:\n",
    "    if model_folder.is_dir():\n",
    "        for train_folder in os.scandir(grid_search_folder + model_folder.name):\n",
    "            if train_folder.is_dir() and train_folder.name == 'models':\n",
    "                plt.close('all')\n",
    "\n",
    "                #Find the best model\n",
    "                for f in os.scandir(grid_search_folder + model_folder.name + '/' + train_folder.name + '/'):\n",
    "                    if f.name[:4] == \"best\":\n",
    "                        saved_model_fname = f.name[:-4]\n",
    "                        print(model_folder.name + \" \" + f.name[:-4])\n",
    "\n",
    "                #Load model\n",
    "                nodes = model_folder.name.split('AE_')[1].split('_')\n",
    "                nodes = [int(x) for x in nodes]\n",
    "                model = module(nodes)\n",
    "                learn = basic_train.Learner(data=db, model=model, loss_func=loss_func, true_wd=True)\n",
    "                learn.model_dir = grid_search_folder + model_folder.name + '/' + 'models/'\n",
    "                learn.load(saved_model_fname)\n",
    "                #model.load_state_dict(torch.load(path_to_saved_model))\n",
    "                learn.model.eval()\n",
    "\n",
    "                # Histograms\n",
    "                idxs = (0, 1000)  # Choose events to compare\n",
    "                data = torch.tensor(test_x[idxs[0]:idxs[1]].values)\n",
    "                #Note, float conversion, this takes time\n",
    "                pred = model(data.float()).detach().numpy()\n",
    "                pred = np.multiply(pred, train_std.values)\n",
    "                pred = np.add(pred, train_mean.values)\n",
    "                data = np.multiply(data, train_std.values)\n",
    "                data = np.add(data, train_mean.values)\n",
    "                \n",
    "                print(hp.heap())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
